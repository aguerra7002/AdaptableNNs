{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptableNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Initializes an adaptable network.\n",
    "        \n",
    "        input_dim (int): gives the dimension of the input\n",
    "        output_dim (int): gives the dimension of the output\n",
    "        initial_size (tuple - int): Gives the initial dimension of each hidden layer. Default 1x32 hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, hidden_size=[32,]):\n",
    "        super(AdaptableNet, self).__init__()\n",
    "        \n",
    "        # Save this information for later\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # This is where we will keep all the layers of the network\n",
    "        self.layers = []\n",
    "        # Initial Layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_size[0]))\n",
    "        # Hidden layers along with output layer\n",
    "        for h in range(len(hidden_size)):\n",
    "            if h == len(hidden_size) - 1:\n",
    "                self.layers.append(nn.Linear(hidden_size[h], output_dim))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_size[h], hidden_size[h+1]))\n",
    "      \n",
    "    \"\"\"\n",
    "        Passes an input through a network and returns the output\n",
    "    \"\"\"\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if l == len(self.layers) - 1:\n",
    "                x = F.log_softmax(x, dim=1)\n",
    "            else:\n",
    "                x = F.relu(x) \n",
    "        return x\n",
    "    \n",
    "    \"\"\"\n",
    "    Increases the number of units in a hidden layer\n",
    "    \"\"\"\n",
    "    def increase_hidden_size(self, h_layer):\n",
    "        # Assert that we have a valid hidden layer\n",
    "        assert(h_layer < len(self.hidden_size))\n",
    "        \n",
    "        # Get the relevant weight matrices.\n",
    "        w1 = self.layers[h_layer].weight\n",
    "        w2 = self.layers[h_layer + 1].weight\n",
    "        bias = self.layers[h_layer].bias\n",
    "        \n",
    "        # add a row/column of 0's to the weight matrices\n",
    "        with torch.no_grad():\n",
    "            # Construct the new weight matrices (simply append a 0 row/column)\n",
    "            new_mat_1 = nn.Parameter(torch.vstack((w1, torch.Tensor(torch.zeros((1, w1.shape[1]))))))\n",
    "            new_mat_2 = nn.Parameter(torch.hstack((w2, torch.Tensor(torch.zeros((w2.shape[0], 1))))))\n",
    "            \n",
    "            # Set the appropriate weights/biases of our network\n",
    "            self.layers[h_layer].bias = nn.Parameter(torch.cat((bias, torch.Tensor([0]))))\n",
    "            self.layers[h_layer].weight = new_mat_1\n",
    "            self.layers[h_layer + 1].weight = new_mat_2\n",
    "            \n",
    "            \n",
    "        return 0\n",
    "    \n",
    "    def decrease_hidden_size(self, h_layer):\n",
    "        \n",
    "        # Assert that we have a valid hidden layer\n",
    "        assert(h_layer < len(self.hidden_size))\n",
    "        \n",
    "        # TODO: Ensure we are not making a dim 0 weight matrix\n",
    "        # Get the relevant weight matrices.\n",
    "        w1 = self.layers[h_layer].weight\n",
    "        w2 = self.layers[h_layer + 1].weight\n",
    "        bias = self.layers[h_layer].bias\n",
    "        \n",
    "        # add a row/column of 0's to the weight matrices\n",
    "        with torch.no_grad():\n",
    "            print(\"W1 & W2 shapes\", w1.shape, w2.shape)\n",
    "            # Row to elim\n",
    "            test_row = torch.argmin(torch.norm(torch.hstack((w1, torch.tranpose(w2, 0, 1)), dim=1)))\n",
    "            print(\"Test Row\", test_row)\n",
    "            row_to_elim = torch.argmin(torch.norm(w1, dim=1))\n",
    "            col_to_elim = torch.argmin(torch.norm(w2, dim=0))\n",
    "            print(\"Row/Col elim:\", row_to_elim, col_to_elim)\n",
    "            \n",
    "            # Construct the new weight matrices (simply append a 0 row/column)\n",
    "            new_mat_1 = nn.Parameter(torch.vstack((w1[:row_to_elim], w1[row_to_elim + 1:])))\n",
    "            new_mat_2 = nn.Parameter(torch.hstack((w2[:,:col_to_elim], w2[:,col_to_elim + 1:])))\n",
    "            print(\"New Mats shape\", new_mat_1.shape, new_mat_2.shape)\n",
    "            # Set the appropriate weights/biases of our network\n",
    "            self.layers[h_layer].bias = nn.Parameter(torch.cat((bias[:row_to_elim], bias[row_to_elim + 1:])))\n",
    "            self.layers[h_layer].weight = new_mat_1\n",
    "            self.layers[h_layer + 1].weight = new_mat_2\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output: tensor([[-1.3467, -1.6033, -1.7231, -1.8845, -1.5690],\n",
      "        [-1.3566, -1.6206, -1.7118, -1.8900, -1.5461],\n",
      "        [-1.3686, -1.6098, -1.7062, -1.8767, -1.5560],\n",
      "        [-1.3450, -1.5980, -1.7339, -1.8994, -1.5563]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "Difference After Increase 1: tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "W1 & W2 shapes torch.Size([8, 12]) torch.Size([10, 8])\n",
      "Row/Col elim: tensor(7) tensor(7)\n",
      "New Mats shape torch.Size([7, 12]) torch.Size([10, 7])\n",
      "Difference After Decrease 1: tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "W1 & W2 shapes torch.Size([7, 12]) torch.Size([10, 7])\n",
      "Row/Col elim: tensor(5) tensor(6)\n",
      "New Mats shape torch.Size([6, 12]) torch.Size([10, 6])\n",
      "Difference After Decrease 2: tensor([[ 0.0009, -0.0069,  0.0017, -0.0018,  0.0055],\n",
      "        [ 0.0287, -0.0043, -0.0142, -0.0150, -0.0074],\n",
      "        [ 0.0165, -0.0024, -0.0081, -0.0085, -0.0042],\n",
      "        [ 0.0194, -0.0030, -0.0098, -0.0103, -0.0052]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_net = AdaptableNet(12, 5, hidden_size=(7, 10))\n",
    "test_data = torch.Tensor(np.random.random((4, 12)))\n",
    "test_output = test_net(test_data)\n",
    "print(\"Test Output:\", test_output)\n",
    "\n",
    "test_net.increase_hidden_size(0)\n",
    "\n",
    "print(\"Difference After Increase 1:\", test_output - test_net(test_data))\n",
    "\n",
    "test_net.decrease_hidden_size(0)\n",
    "\n",
    "print(\"Difference After Decrease 1:\", test_output - test_net(test_data))\n",
    "\n",
    "test_net.decrease_hidden_size(0)\n",
    "\n",
    "print(\"Difference After Decrease 2:\", test_output - test_net(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
